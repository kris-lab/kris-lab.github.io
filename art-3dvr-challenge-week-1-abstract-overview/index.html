<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Art 3D/VR challenge - week 1 - abstract overview - __Chris_Stasiak__</title><meta name="description" content="Previous weekNext weekIntroduction This article is continuation of Art 3D/VR challenge. Please note, I don't describe all possible technologies, knowledge, science, models, and approaches you can find on the search engine. Index Abstract segments The chart above shows the most important 6&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://blog.kris-lab.com/art-3dvr-challenge-week-1-abstract-overview/"><link rel="alternate" type="application/atom+xml" href="https://blog.kris-lab.com/feed.xml"><link rel="alternate" type="application/json" href="https://blog.kris-lab.com/feed.json"><meta property="og:title" content="Art 3D/VR challenge - week 1 - abstract overview"><meta property="og:image" content="https://blog.kris-lab.com/media/posts/889/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd.png"><meta property="og:site_name" content="__Chris_Stasiak__"><meta property="og:description" content="Previous weekNext weekIntroduction This article is continuation of Art 3D/VR challenge. Please note, I don't describe all possible technologies, knowledge, science, models, and approaches you can find on the search engine. Index Abstract segments The chart above shows the most important 6&hellip;"><meta property="og:url" content="https://blog.kris-lab.com/art-3dvr-challenge-week-1-abstract-overview/"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@_krislab"><meta name="twitter:title" content="Art 3D/VR challenge - week 1 - abstract overview"><meta name="twitter:description" content="Previous weekNext weekIntroduction This article is continuation of Art 3D/VR challenge. Please note, I don't describe all possible technologies, knowledge, science, models, and approaches you can find on the search engine. Index Abstract segments The chart above shows the most important 6&hellip;"><meta name="twitter:image" content="https://blog.kris-lab.com/media/posts/889/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd.png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:var(--body-font);--logo-font:var(--body-font);--menu-font:var(--body-font)}</style><link rel="stylesheet" href="https://blog.kris-lab.com/assets/css/style.css?v=942a7248764f7bd78c8eab338e2f678a"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.kris-lab.com/art-3dvr-challenge-week-1-abstract-overview/"},"headline":"Art 3D/VR challenge - week 1 - abstract overview","datePublished":"2016-02-29T20:59","dateModified":"2020-10-21T23:15","image":{"@type":"ImageObject","url":"https://blog.kris-lab.com/media/posts/889/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd.png","height":1038,"width":1600},"description":"Previous weekNext weekIntroduction This article is continuation of Art 3D/VR challenge. Please note, I don't describe all possible technologies, knowledge, science, models, and approaches you can find on the search engine. Index Abstract segments The chart above shows the most important 6&hellip;","author":{"@type":"Person","name":"Chris Stasiak"},"publisher":{"@type":"Organization","name":"Chris Stasiak"}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-176563351-1"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-176563351-1');</script></head><body><header class="header" id="js-header"><a href="https://blog.kris-lab.com/" class="logo">__Chris_Stasiak__</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li class="home-button"><a href="https://blog.kris-lab.com" target="_self">{ home }</a></li><li class="has-submenu"><a href="https://blog.kris-lab.com/" target="_self" aria-haspopup="true">#tagged</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://blog.kris-lab.com/tag/news/" target="_self">News</a></li><li><a href="https://blog.kris-lab.com/tag/devops-2/" target="_self">DevOps</a></li><li><a href="https://blog.kris-lab.com/tag/ffmpeg/" target="_self">Media</a></li><li><a href="https://blog.kris-lab.com/tag/vr/" target="_self">360/3D/VR</a></li><li><a href="https://blog.kris-lab.com/tag/artificial-intelligence-2/" target="_self">AI/ML/TPU</a></li><li><a href="https://blog.kris-lab.com/tag/tools/" target="_self">Tools</a></li></ul></li><li><a href="https://github.com/kris-lab" target="_blank">GitHub</a></li><li><a href="https://www.linkedin.com/in/krzysztof-chris-stasiak-6261244b/" target="_blank">LinkedIn</a></li><li><a href="https://keybase.io/krislab" target="_blank">Keybase</a></li><li class="has-submenu"><span class="is-separator" aria-haspopup="true">...</span><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://twitter.com/_krislab" target="_blank">Twitter</a></li><li><a href="https://www.vivino.com/users/krislab" target="_blank">Vivino</a></li><li><a href="https://you.23andme.com/published/reports/4ce9d90ae16a442590f1d3f8dcfd0eca/?share_id&#x3D;efcd35acba5a4f77" target="_blank">23andMe</a></li></ul></li><li class="line-vertical-separator"><span class="is-separator">|</span></li><li class="has-submenu"><a href="https://blog.kris-lab.com/contact/" target="_self" aria-haspopup="true">About</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://blog.kris-lab.com/contact/" target="_self">Contact</a></li><li><a href="https://blog.kris-lab.com/resume/" target="_self">Resume</a></li><li><a href="https://blog.kris-lab.com/references/" target="_self">References</a></li><li><a href="https://blog.kris-lab.com/gallery/" target="_self">Photos</a></li><li class="line-horizontal-separator"><span class="is-separator">-</span></li><li><a href="https://blog.kris-lab.com/legal/" target="_self">Legal</a></li></ul></li></ul></nav><div class="search"><div class="search__overlay js-search-overlay"><form action="https://blog.kris-lab.com/search.html" class="search__form"><input class="search__input js-search-input" type="search" name="q" aria-label="Search input" placeholder="search..." autofocus="autofocus"> <button class="search__submit">Search</button></form></div><button class="search__btn js-search-btn" aria-label="Search"><svg role="presentation" focusable="false" height="18" width="18"><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#search"/></svg></button></div></header><main><div class="wrapper"><article class="post"><header class="post__header"><a href="https://blog.kris-lab.com/tag/camera/" class="post__maintag">Camera</a><h1 class="post__title">Art 3D/VR challenge - week 1 - abstract overview</h1><div class="post__meta"><div class="post__author">By <a href="https://blog.kris-lab.com/authors/chris-stasiak/" class="invert" rel="author" title="Chris Stasiak">Chris Stasiak</a></div><time datetime="2016-02-29T20:59">February 29, 2016</time><div class="post__comments"><svg aria-hidden="true"><title>Comments</title><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#comments"/></svg> <a href="https://blog.kris-lab.com/art-3dvr-challenge-week-1-abstract-overview/#comments" data-disqus-identifier="889" class="invert" rel="nofollow">0</a></div></div></header><figure class="post__featured-image"><img src="https://blog.kris-lab.com/media/posts/889/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd.png" srcset="https://blog.kris-lab.com/media/posts/889/responsive/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd-xs.png 300w, https://blog.kris-lab.com/media/posts/889/responsive/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd-sm.png 480w, https://blog.kris-lab.com/media/posts/889/responsive/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd-md.png 768w, https://blog.kris-lab.com/media/posts/889/responsive/gxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd-lg.png 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="eager" height="1038" width="1600" alt=""></figure><div class="post__inner"><div class="post__entry"><table style="font-size: 14px; border: none;"><tbody><tr><td><a href="https://blog.kris-lab.com/art-challenge-with-ultrahd4k-3dvr-hardware-accelerated-relatime-hdmirtpwebrtc-live-streaming-and-communication-portable-device/" title="ART-Challenge with UltraHD/4K + 3D/VR hardware accelerated, relatime HDMI/RTP/WebRTC live streaming and communication – portable device">Previous week</a></td><td style="text-align: right;"><a href="https://blog.kris-lab.com/art-3dvr-challenge-week-2-looking-for-performance/" title="Art 3D/VR challenge – week 2 – looking for performance">Next week</a></td></tr></tbody></table><p><strong>Introduction</strong></p><p>This article is continuation of <a href="https://blog.kris-lab.com/intelligent-content-streaming-engine-icse-part-1-introduction-51/" title="ART-Challenge with UltraHD/4K + 3D/VR hardware accelerated, relatime HDMI/RTP/WebRTC live streaming and communication – portable device">Art 3D/VR challenge</a>.</p><p>Please note, I don't describe all possible technologies, knowledge, science, models, and approaches you can find on the search engine.</p><hr><h6>The following information is my personal experience I have gained over the last 10+ years of my career. BY applying PRESENTED models and stack I can guarantee to achieve FAIR ENOUGH results.</h6><hr><p><strong>Index</strong></p><ol><li><a href="#brief-theory">Brief theory</a><ol><li><a href="#brief-theory-abstract">Abstract segments</a></li><li><a href="#brief-theory-non3d">Monoscopic vision capturing models and constructions</a><ol><li><a href="#brief-theory-non3d-360">360° capturing model</a></li><li><a href="#brief-theory-non3d-sphere">Sphere capturing model</a></li></ol></li><li><a href="#brief-theory-3d-models">Stereoscopic vision capturing models and constructions</a><ol><li><a href="#brief-theory-vr-real">Full Real Image capturing and rendering</a></li><li><a href="#brief-theory-vr-green">Green-Screen capturing and rendering model</a></li><li><a href="#brief-theory-vr-soft-intelligent">Intelligent Chroma Keying capturing and rendering model</a></li></ol></li><li><a href="#brief-theory-equidistant">Equidistant stereo rendering vs Equidistant mono rendering</a></li><li><a href="#brief-theory-hud">VR HUD</a></li><li><a href="#brief-theory-summary">Summary</a></li></ol></li><li><a href="#hardware">Hardware</a></li><li><a href="#software">Software and standardisation</a></li><li><a href="#infrastructure">Infrastructure and scalability</a></li><li><a href="#summary">Summary</a></li></ol><p><a name="brief-theory"></a></p><h3 style="text-align: center;">BRIEF THEORY</h3><p><a name="brief-theory-abstract"></a><br><strong>Abstract segments</strong></p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/OVXT-eTTz6Gpob6HdIdha4TQNl-_f-ZG0zF3O3CPVAIE_5J_msML8qg8RwAP7B2sYbdVXtNgGr3KfOQvud_ersrC2cCLj2fLu0bo3kgvBj5myH4hV7FLgCe4H7--NYOBMaNNFJih" alt="" width="624" height="585"></figure><p><span data-preserver-spaces="true">The chart above shows the most important </span><strong><span data-preserver-spaces="true">6 problems in 4 groups</span></strong><span data-preserver-spaces="true"> we would have to solve on the way from capturing a real scene up to the output as network packets or dedicated media interface. All groups will be described in more detail in the current blog post and the following one.</span></p><p>Brief definition of segments:</p><ul><li><strong>Capturing</strong> - this segments describes lenses, cameras, mount setup, media format and output interface. This is important to provide the highest quality of image with the lowest possible noise. It also requires to provide the precision of optics and calibration for stereoscopic pairs of cameras. Low latency video processing (DSP), media format (RAW, MPEG) and output (USB/CSI, MPEG, RAW: 8/10/16 bits) will be very crucial on the media pipeline.</li><li><strong>Stitching</strong> - this segments describes techniques of media concatenation and multiplexing. By choosing the right techniques we will directly influence computation complexity, encoding latency and outgoing bitrate. It is very important to fully optimise this step. That process can be done on software level, on software/hardware level (GPU) and on hardware only level (e.g. CSI-2 multiplexer).</li><li><strong>Processing</strong> - this segments describes the set of techniques for multimedia processing (FFmpeg, Gstreamer)</li><li><strong>Controlling</strong> - this segments describes core business logic, APIs, provisioning, management and dynamic media pipeline optimisation (REST, Puppet, Ruby, GStreamer, FFmpeg etc.)</li><li><strong>Encoding</strong> - this segment describes techniques for software and hardware video/audio encoding (VP8/9, h264, AAC, MP3, Opus). It can be achieved by software, software/hardware (Video chipset / GPU) and dedicated hardware chipsets (e.g. DaVinci, OMAP) approach.</li><li><strong>Streaming</strong> - this segments describes techniques for integrating video/audio into media containers for network streaming (RTP, WebRTC) and digital media interfaces<span style="font-size: 18px;"> (DVI, HDMI, CSI, SDI). This process is relatively cheap in computation and can be done by software for network output and by dedicated hardware for analog/digital media interfaces.</span></li></ul><p>By introducing <strong>4 categories</strong> of problems I would like to put some light on the scalability and computing power distribution issues. Many use cases require sophisticated image processing, encoding and streaming which cannot be easily achieved on single CPU or GPU. By grouping specific problems we can distribute/clone video and audio on the computing cluster and distribute tasks by role. I will explain scalability problem in more details below and following blogposts. For now, please notice that the computing power is the top feature to achieve real time streaming and communication abilities.</p><p>Please note that the last segment of pipeline (which is not included in the chart) would be <strong>playback</strong>. I will explain the problem of playback and rendering to some extend below. However network latency, decoding, playback and VR/3D rendering is not the main topic of this challenge. I will provide external resources in this area of expertise.</p><p>Let's have a look on some vision theory...</p><p><strong style="font-size: 18px;">Monoscopic vision rendering models and constructions</strong></p><p><a style="font-size: 18px; background-color: white;" name="brief-theory-non3d-sphere"></a><a style="font-size: 18px; background-color: white;" name="brief-theory-non3d-sphere"></a></p><p><a href="https://en.wikipedia.org/wiki/Monocular_vision">Monocular vision</a> is <a title="Visual perception" href="https://en.wikipedia.org/wiki/Visual_perception">vision</a> in which both <a title="Human eyes" href="https://en.wikipedia.org/wiki/Human_eyes">eyes</a> are used separately. By using the eyes in this way, as opposed by <a title="Binocular vision" href="https://en.wikipedia.org/wiki/Binocular_vision">binocular vision</a>, the <a title="Field of view" href="https://en.wikipedia.org/wiki/Field_of_view">field of view</a> is increased, while <a title="Depth perception" href="https://en.wikipedia.org/wiki/Depth_perception">depth perception</a> is limited. The eyes of an animal with monocular vision are usually positioned on opposite sides of the animal's head, giving it the ability to see two objects at once.</p><p>Monocular vision affects how the brain perceives its surroundings by decreasing the available visual field, impairing peripheral vision on one side of the body, and compromising depth perception, all three of which are major contributors to the role of vision in balance.</p><p>A monoscopic camera is characterized by having just one lens as opposed to stereoscopic cameras that have two:</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/c29MpaNK39vj8Qo5h8vsw6yXXdge62SwsfSty94HygJEI03crCEp3hQ4ORHtQfdCKqt6OZ3wlee1qzcI3cPWneyLYC0o5K4L4R33PsDf7b_NOdwUSn0MDAOKdsqp8Y3Rg2ZOcJX0" alt="" width="230" height="262"></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/X9Fqb9WHsdyNfvSkS1blszQMiRMmcgKkWK6huSCrZZEQwRzug0EFq-WyagZppdDGTE9Bm-o6mrHN-UoYtxEWWdvQz-PLCauCIWxFKJI9DOZJuONeKEndjuN_XRcV-4Y0x8Ld6aAI" alt="" width="287" height="215"></figure><p>Capturing models:</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/vSJnX7z8KWX7XLP-yM_01sZqmpBSc29HHX0ATDLg_1hFCCH29bJ6IFsdOHHg7z0pS6J8xtACTdUVc1gfdS-VT_EZttU4SOe7ean8LR3hc6MwIslXzaD3MGswnPxwvKve_DwYYI3r" alt="" width="624" height="208"></figure><p style="text-align: center;">(source: google engine)</p><p><a name="brief-theory-non3d-360"></a></p><ul><li>360° capturing model</li></ul><p>A 360° video is simply a flat equirectangular video that is morphed into a sphere for playback on a VR headset. If a 360° video is monoscopic, it means that both eyes see a single flat image, or video file.</p><p>Monoscopic 360° videos are currently the most commonly filmed content for VR. This type of video is usually filmed with a single camera per field of view (FOV) and stitched together to form a single equirectangular video. Monoscopic videos have fewer technical challenges and are the easiest and cheapest to produce</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/Z78JAGSSzW3-lehNJ3iJDY8GMfNsPiuqpyQsLfzuLmXvRd8l4D8j8YqpGx-tuIu_BHyLfKBTiv7M5WfkjNsHZ5O_zevYEbeH5j5lXlnhxiC04ODAW-fWVN8lscIfhwnFgK2hliwc" alt="" width="624" height="196"></figure><p>(source: google engine)</p><ul><li>Sphere capturing model</li></ul><p>The concept behind the spherical panoramas is nothing complicated, this technique is meant to reproduce in virtual reality (VR) the first person view taken in any direction at a certain instant. In simple words you have to take a lot of pictures all around, at full 360° on the horizon and also up (<a href="http://en.wikipedia.org/wiki/Zenith" rel="nofollow">zenith point</a>) and down (<a href="http://en.wikipedia.org/wiki/Nadir" rel="nofollow">nadir point</a>), and merge them into a single image.</p><p>Of course the problems appear when you try to merge the pictures together, indeed they have to be warped to match each other. A software could do that easily, but to do that it has to recognize the identical details in different images to overlap them. These identical details are named "control points" (CP). The more you overlap sequential images, the more CP you'll have. To be fair the number of CP is not as essential as their "quality".</p><p>Stitching a set of photos in a uniform single equirectangular image is totally a mathematical issue. In case you wondered what <a href="http://en.wikipedia.org/wiki/Equirectangular_projection" rel="nofollow">equirectangular</a> means, this projection type is the most used way in VR to display a sphere on a plane, and if you'll manage to load it in a panorama viewer it will be certainly supported.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/v_o7tVModm23qgh1Hp1Al8_1oHCBCPFLXiSx3qONqZr4-7pUwbxGto56TiXhMVrA5QWQv0YoPO4NuYp3tfaPgyL7gtQpPwHy3K7tiDuUPqbHCc0G6fgXpcundxsvj-ZJSyubJ99q" alt="" width="624" height="565"></figure><p style="text-align: center;">(source: google engine)</p><p><a name="brief-theory-3d-models"></a><br><strong>Stereoscopic vision capturing models and constructions<br></strong></p><p>Stereoscopic videos are usually filmed with two cameras per field of view or one camera mapped to each eye, giving the perception of depth. While this experience is great when done correctly, it is much harder to get right. You will need to stitch camera footage for each eye separately and then create a side-by-side (SBS) 3D video mapping the left and right video to each eye. The SBS 3D video comes in a few configurations—top and bottom or right and left side-by-side. It is important to note that stereoscopic 3D decreases video resolution because the two side-by-side videos split the resolution of the screen.</p><p><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/MwhAqhujuADCbA89f9BU1fJJlgPxe-VuFenO3A4LOp7LfAXh6Zpg_M0fLZjMgGLNRW3Ygw5VD2QcI3gi7qV2T9VDW4Go5ADpHQyLcevAlm-aA_SH3ldQHMwl1g5RAaI5iQ-I2CWa" alt="" width="314" height="221"><img loading="lazy" style="font-style: inherit;" src="https://blog.kris-lab.com/media/posts/889/K0iPvvs7ovIKKAnRmkaKuoXw_gZDUS2ALxGd_F44Cz_9jgGKdrK3hEsBGTHijYyo8LwStdRCwEVjOp1J7vfjSuHtEW1gBkAdjv7wC888enWrQ1xm6oueqwqQIWvx7OAyrTf6vZIL" alt="" width="278" height="202"></p><p><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/qVXO_L51lcyH-LqTHhwunm6WUhxdvn4aEme6M8YP_ti0Dy0gnZsZ2KR6MAM4cjb_B3OCoZ-G4uKDzhPKiF3o32ygMQ7qs7n076-aoEAkgJOhZdl9eYUbLH7ecWA5WWZge31TmhsS" alt="" width="266" height="219"><img loading="lazy" style="font-style: inherit;" src="https://blog.kris-lab.com/media/posts/889/D2fwcev4OskNArMGLn0_iOLBMILGFtzUXA5dKpyx2oQz_h4ttK4eQrHKyye3NDIgTZNiBwb9_yY7oNVBSW-3k5RrzmWG3Ax10i3XsV32aYOlaz9AWorT8-d3nV5ndST8Eh3aeD0f" alt="" width="313" height="260"></p><p><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/erAWT2U-RFd0VYrfjn81J-Y5WHaZHV8K212yYivftPf4JtPenJ7GJfxf6slW_Hwpqu4Jh0-oBuOQ-62JK5Y75EzCsnWlT0Uh6fgLZlY_ObgYHPYr_kfR5jp58-ofc0SArOb9Cwq7" alt="" width="316" height="210"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/KvgoysDMgFuU_D7SsXqImMQuSSiJzTz8hr4CwIKm_y5DAnz99-AEibTzSLqebCTEFnX233j-9TATY2CzpU8H2vFrDygVFVWD3pzp1JX4rV3owTn7B76QEr2ZQ-_xXElgfZ3B5oAe" alt="" width="292" height="173"></p><p style="text-align: center;">(source: google engine)</p><hr><p><em style="color: #ff0000; font-size: 18px;">Stereoscopic capturing can work with <strong>360°, </strong><strong>sphere</strong> or <strong>wide angle</strong> field of view. I will focus on <strong>wide angle</strong> capturing for simplicity as this is the case we would implement in the first generation of our device. Also I will present some concepts and tricks used by Virtual Reality products to gain lower bitrate, higher quality and better user experience overall.</em></p><hr><p>Let's have a look on some of capturing models. There are many of concepts and tricks you could apply to achieve a very good results. There are models which fit very good for real-time communication, live streaming or simply recording. Latency of network, bitrate, encoding/decoding complexity and playback are the most common aspect we have to keep in mind. I will focus on models for real-time communication and live streaming on the first place.<br><a name="brief-theory-vr-real"></a></p><ul><li><strong>Full Real Image</strong> capturing and rendering</li></ul><p>If a video is stereoscopic, it means there are two videos, one mapped to each eye, providing depth and 3D appearance.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/ZzZX9mI_NAKUVaAXc6AKuHBpug9FQ27Y9mztQC4rRX6tXb-UuFQsx6Bs0JZZnEIlLsHMH9fBX4j8yHqtzbUNaNRtRoQ0NpxzVOEzN6q3UVGkH5OLaW0uaOQRBAZBS-MGjrqMJAkW" alt="" width="624" height="311"></figure><p style="text-align: center;">(source: google engine)</p><p><a name="brief-theory-vr-green"></a></p><ul><li><span style="font-size: 18px;"> <strong>G</strong></span><strong>reen-Screen</strong> capturing and rendering model</li></ul><p>Chroma key compositing, or chroma keying, is a <a title="Special effects" href="https://en.wikipedia.org/wiki/Special_effects">special effects</a> / <a title="Post-production" href="https://en.wikipedia.org/wiki/Post-production">post-production</a> technique for <a title="Compositing" href="https://en.wikipedia.org/wiki/Compositing">compositing</a> (layering) two <a title="Image" href="https://en.wikipedia.org/wiki/Image">images</a> or <a title="Video" href="https://en.wikipedia.org/wiki/Video">video</a> streams together based on color hues (<a title="Colorfulness" href="https://en.wikipedia.org/wiki/Colorfulness">chroma</a> range). The technique has been used heavily in many fields to remove a <a title="wikt:background" href="https://en.wiktionary.org/wiki/background">background</a> from the subject of a photo or video – particularly the <a title="News" href="https://en.wikipedia.org/wiki/News">newscasting</a>, <a title="Motion picture" href="https://en.wikipedia.org/wiki/Motion_picture">motion picture</a> and <a title="Videogame" href="https://en.wikipedia.org/wiki/Videogame">videogame</a> industries. A color range in the top layer is made transparent, revealing another image behind. The chroma keying technique is commonly used in <a title="Video production" href="https://en.wikipedia.org/wiki/Video_production">video production</a> and post-production. This technique is also referred to as <strong>color keying</strong>, <strong>colour-separation overlay</strong> (<strong>CSO</strong>; primarily by the <a title="BBC" href="https://en.wikipedia.org/wiki/BBC">BBC</a><sup id="cite_ref-What_is_Chroma_Key.3F_2-0"><a href="https://en.wikipedia.org/wiki/Chroma_key#cite_note-What_is_Chroma_Key.3F-2">[2]</a></sup>), or by various terms for specific color-related variants such as <strong>green screen</strong>, and <strong>blue screen</strong> – chroma keying can be done with backgrounds of any color that are uniform and distinct, but green and blue backgrounds are more commonly used because they differ most distinctly in hue from most <a title="Human skin color" href="https://en.wikipedia.org/wiki/Human_skin_color">human skin colors</a>.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/lfC_v_kmcbuJeDLa_mZZMFi2_eAwyM4gT8p7fsdyEuKzsGEekKgfkb8IBXAcjsZLuhEDQs7cPFx4lWm1oP2wxaIRTtZeFQgxhy4hkuGgZ48GmPt4NGrS5ChGGqOdfQ6j5XPYl5_s" alt="" width="500" height="375"></figure><p style="text-align: center;">(source: google engine)</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/QsEO41qRbmAbpd6Q1_EPweAdH1_DIXZ8fVCoidc7zQhaO7rewMtNEuIL0ENjGzofWPIJuDeiCy4_IxUQdjxUvHCtef2ZmBV7U1PqeQIbSsWXtaSHoTjy3OWytMrGNd-eE-BOf5zZ" alt="" width="624" height="176"></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/YrdsapXKPJeBdA0Ffh7knTC4jguv6_ir6VocaU_nBooMlwNicoujygQuFN3aAET_aSi0c_z8xSmo0sB9lL1Dy1lBzrUQYMeNHdgGtodXuOHExvaY6kM72_0whtlzJFpfDsE-Ps2n" alt="" width="624" height="391"></figure><p style="text-align: center;">(source: alicex.com)</p><p style="text-align: left;">This method is very popular and works perfectly for TV, movies industry etc. However for Virtual Reality effects and products it is not the easiest solution. It can work correct with dedicated setup but will not work in any random place and not in changing/mobile environment. Also complexity of the setup it too high to provide platform for start-ups and seed ideas.</p><hr><p><span style="color: #ff0000;">Considering limitation of regular green studio setup I would like to propose next generation of "green studio" which would fit perfectly into <strong>VR</strong> environment...</span></p><hr><p><a name="brief-theory-vr-soft-intelligent"></a></p><ul><li><strong>Intelligent Chroma Keying </strong>capturing and rendering model</li></ul><p>I would like to specifically focus on this topic. The "Intelligent Chroma Keying" is not common used terminology. Regular green screen recording uses chroma keying for extraction green colour for further video processing.</p><p>Computing vision algorithms bring a new layer of image processing. It allows for accurate extraction of objects and accurate humans and objects tracking in real time. Integrating the <strong>OpenCV</strong> support for face tracking, objects tracing, template matching, pre-learning and grab-cut algorithms we can extract background/foreground from interesting us actions!</p><p>There are many methods for image segmentation. Below some of them:</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/7jC2SneNLt9VftRdLXJN9jRyhS2EDhSPkYRcZw033WQ5owjeKI2BO8zHajhzH3VaUjhw6YnhAb3HyvuMFZ5N2hOF_fgTvX0jXf1MP_Q2uTT3JcabA5RiWwD_ugVCnUXseygLB3S8" alt="screen shot 2016-01-10 at 10 17 33" width="624" height="275"></figure><p style="text-align: center;">(source: google engine)</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/rsBoy5vX2no7QZpy3quI75CB5DslyVikZfOlzlkRBt1jliGsAzRDqaZQmAT8LxE5libKTx0O8qDbB1ZYV86S-YH9O7dCJshVBUlyaPbuE1ePV4Vei9pWQzsLGIigpWWoxxPXuoGk" alt="" width="485" height="243"></figure><p style="text-align: center;">(source: google engine)</p><p>To achieve a higher level of accuracy I recommend to use <strong>depth camera</strong> which provides additional axis of information. It also significantly speeds up computation.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/iow-kpknt9BVoGQh6AiH2b0OH_hLtuY5nL5EXdZKyUbkkLTokV9cI5ZePWRBC4J7YTz45fU-CiHQp0KrWCPHpJsdVxqc8U4HW-zWCaX5lHpeT30mUQ1cmWfoxDlVpOjebzjDFyo9" alt="" width="624" height="232"></figure><p style="text-align: center;">(source: seas.harvard.edu)</p><p style="text-align: left;">Next level of accuracy would introduce <strong>deep learning algorithm</strong> (e.g. DNN, ANN) for better understanding of objects in the scene.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/GiLfoaY5cSOJ3Hl8dFaDEdGJqNlsgJ9-mYEh8YxC3TbGySnEcl5YJK4B5lTg1abZ_do5hptvT9a04UCohY2v3CJPGs8EQrfKfxm2SDjaghYkCnUjFUF2VidgdlYy3e7dUSQ5-rK8" alt="" width="624" height="335"></figure><p style="text-align: center;">(source: google engine)</p><p style="text-align: left;">Unfortunately, the "deep learning" requires a lot of computing power. It will require another hardware board which would provide accurate information about the scene in real time.</p><hr><p style="text-align: left;">Combining the simple faces and objects tracking, deep learning objects recognition, depth information and grab-cut algorithm we can implement a very accurate logic for removing a background and providing runtime information in real-time.</p><hr><p style="text-align: justify;"><span style="color: #ff0000;">AT THE END OF THE DAY THE <strong>"<em>INTELLIGENT CHROMA KEYING</em>"</strong></span> <span style="color: #ff0000;">IS ONE OF MAIN GOAL FOR THIS CHALLENGE</span>. <span style="color: #ff0000;">IT WILL ALLOW TO STREAM <strong>VR</strong> <strong>MEDIA</strong> FROM ANY ENVIRONMENT, REMOVE GREEN SCREEN COMPLEXITY, LOWER BITRATE, SAFE COMPUTING POWER ON ENCODING/DECODING AND PROVIDE EXTRA REALTIME INFORMATION ABOUT SCENE AND ACTIONS, AS WELL AS ALLOWING TO INTEGRATE <strong>VR MEDIA</strong> IN ANY VIRTUAL SCENERY ON THE PLAYBACK END POINT.</span></p><hr><p><a name="brief-theory-equidistant"></a><br><strong>Equidistant stereo rendering vs Equidistant mono rendering</strong></p><p>An equidistant stereo lens shader includes a user controlled device and an output device. The user controlled device includes one or more controllers, which are configured for orientation of a stereo field, orientation of a projection surface relative to a camera, use of converging stereo rays or parallel stereo rays, control over the zero-parallax distance for converging rays, and control over the interocular distance. The output device is configured to output left-eye images, right-eye images, or monoscopic images as equidistant fisheye projections.</p><p>Claims:</p><p><strong>1.</strong> An equidistant stereo lens shader, comprising: a user controlled device, including one or more controllers configured for orientation of a stereo field, orientation of a projection surface relative to a camera, use of converging stereo rays or parallel stereo rays, control over the zero-parallax distance for converging rays, and control over the interocular distance; and an output device that outputs left-eye, right-eye, or monoscopic images as equidistant fisheye projections.</p><p><strong>2.</strong> An equidistant lens shader process, comprising the steps of: controlling orientation of a stereo field; controlling orientation of a projection surface relative to a camera; controlling use of converging stereo rays, or parallel stereo rays; controlling a zero-parallax distance for converging rays; controlling an interocular distance; and outputting left-eye, right-eye, or monoscopic images as equidistant fisheye projections with the equidistant stereo lens shader of claim <strong>1.</strong></p><p>Read more:</p><ul><li><a style="font-family: sans-serif; font-style: normal;" href="http://www.patentsencyclopedia.com/app/20160007014#ixzz41ebVT7CN" target="_blank" rel="noopener">http://www.patentsencyclopedia.com/</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="http://www.google.com/patents/WO2014145558A1?cl=en" target="_blank" rel="noopener">http://www.google.com/patents/</a></li></ul><p><a name="brief-theory-hud"></a><br><strong>VR HUD</strong></p><p>Most up-to-date virtual realities are displayed either on a computer screen or with an HD VR special <a title="Stereoscopy" href="https://en.wikipedia.org/wiki/Stereoscopy">stereoscopic displays</a>, and some simulations include additional sensory information and focus on real sound through speakers or headphones targeted towards VR users. The simulated environment can be similar to the real world in order to create a <a title="Lifelike experience" href="https://en.wikipedia.org/wiki/Lifelike_experience">lifelike experience</a>.</p><figure class="post__image post__image--center">The simples HUD would be cardboard viewer:<br><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/m_hZ2rNVuS-3T9Aop41QhT3c4FVffzExRfnzNy1FuCPneRKJd6xyCpdE_1vNyac86VJFZjJEjTLfGXGO3_SngFMpo5Wodp5nGXdLk-baculfvtMdSCvxcy6p1cRskZymbBE2-Gkj" alt="" width="364" height="364"></figure><p><a name="brief-theory-summary"></a><br><strong>Summary - Where to go from here?</strong></p><p>I recommend to have a look on <a href="http://elevr.com/blog/">eleVR</a><strong> </strong>blogpost and <a href="http://elevr.com/resources/">github</a> source. There are tons of very useful informations, experiments, benchmarks, videos and articles. See some of them:</p><ul><li><a href="http://elevr.com/tutorial-3d-spherical-camera-head/">http://elevr.com/tutorial-3d-spherical-camera-head/</a></li><li><a href="http://elevr.com/this-is-what-science-sounds-like/">http://elevr.com/this-is-what-science-sounds-like/</a></li><li><a href="http://elevr.com/stereo-polygons/">http://elevr.com/stereo-polygons/</a></li><li><a href="http://elevr.com/cg-vr-1/">http://elevr.com/cg-vr-1/</a></li><li><a href="http://elevr.com/dont-look-down/">http://elevr.com/dont-look-down/</a></li><li><a href="http://elevr.com/updates-webvr-phonevr-wearality-kickstarter-etc/">http://elevr.com/updates-webvr-phonevr-wearality-kickstarter-etc/</a></li><li><a href="http://elevr.com/elevrant-360-stereo-consumer-cameras/">http://elevr.com/elevrant-360-stereo-consumer-cameras/</a></li><li><a href="http://elevr.com/10-fun-and-easy-things-that-anyone-can-make-for-vr/">http://elevr.com/10-fun-and-easy-things-that-anyone-can-make-for-vr/</a></li><li><a href="http://elevr.com/are-you-elevranting-or-am-i-projecting/">http://elevr.com/are-you-elevranting-or-am-i-projecting/</a></li><li><a href="http://elevr.com/editing-spherical-3d-in-premiere-and-after-effects-a-design-document/">http://elevr.com/editing-spherical-3d-in-premiere-and-after-effects-a-design-document/</a></li></ul><p><a name="hardware"></a></p><h3 style="text-align: center;">HARDWARE</h3><p><strong>Lenses</strong></p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/eHiJqWGBZ7C-k1-0am_hj-lpS4aEJMtQkRnsCBSeg7aLzCmCuRfnUQ5TK3MZDxhPEyg66gvoCRpm4MA5hDkLsL6NcTWqVE42pqNOfYSxk6cpwXYaVjXWDfwrxvq4Tjs8kMStfKNh" alt="" width="274" height="262"></figure><p style="text-align: center;">(source: google engine)</p><ul><li>Mount type: M12 recommended for the portable, small device</li><li>Distortion level: should be lowest possible</li><li>FOV: horizontal, vertical should be highest possible 180°+ with minimal distortion</li><li>Resolution: 5M+ is recommended</li></ul><p><strong>Camera</strong></p><ul><li>Noise: lowest possible</li><li>Sensitivity: highest possible</li><li>Focus: manual should be efficient</li><li>Interface: USB 3.0 or CSI-2</li><li>Output format: RAW recommended</li></ul><p><strong>CPU</strong></p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/6BIZD1bO4KPH_i3txIsB-o_SExq3qimKCDxhgKW1RZiWUMujJ6TeLvYVIvPQCCwagM8gMDUrYS9E1JAqUCNDRvRj0EnOySrCl-_GoIjZjtwQy2WYl0XtyyalstVaOft6rRNSVVG1" alt="" width="342" height="257"></figure><p style="text-align: center;">(source: google engine)</p><ul><li>ARM Cortex-7, ARM Cortex-9, <a href="https://en.wikipedia.org/wiki/ARM_Cortex-A15">ARM Cortex-15</a>, <a href="https://en.wikipedia.org/wiki/ARM_Cortex-A57">ARM Cortex-57</a></li><li>Intel <a href="https://en.wikipedia.org/wiki/Skylake_(microarchitecture)">Skylake</a>, <a href="https://en.wikipedia.org/wiki/Broadwell_(microarchitecture)">Broadwell</a></li><li><a href="https://en.wikipedia.org/wiki/Xeon">Intel Xeon</a></li></ul><p><strong>GPU</strong></p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/JCriRAe3N5c5MDca3X7rPhwca_itgBWYeJ4XczvlWTKqlZF6nS7QTAeZOLLt22eWFp7zg45DGrNLExz7_WegDBYkPRVy3DLdj4dIQ1C1s9E2OwI10RB_bmqek93qJ5oRK8zujFg0" alt="" width="362" height="218"></figure><p style="text-align: center;">(source: nvidia.com)</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="Intel HD" href="https://en.wikipedia.org/wiki/Intel_HD_and_Iris_Graphics">Intel HD</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="Nvidia Tegra" href="https://en.wikipedia.org/wiki/Tegra">Nvidia (Tegra)</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="AMD Radeon" href="https://en.wikipedia.org/wiki/Radeon">AMD (Radeon) </a></li></ul><p>Any of the latest chipsets will work for this project. We have to make sure that the size of this hardware is reasonable (small fan or radiator).</p><p><strong>Interfaces</strong></p><ul><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/HDMI">HDMI-in, HDMI-out</a>, <a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Serial_digital_interface">SDI</a>, <a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Camera_Serial_Interface">CSI-2</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/USB_3.0">USB3</a>, <a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Gigabit_Ethernet">Gigabit Ethernet</a>, <a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Wi-Fi">Wifi</a>, <a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Bluetooth">Bluetooth</a></li></ul><p><strong>Hardware acceleration</strong></p><p>Chipset for encoding, decoding and video transformations.</p><ul><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Texas_Instruments_DaVinci">DaVinci</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/OMAP">OMAP</a></li></ul><p><a name="software"></a></p><h3 style="text-align: center;">SOFTWARE AND STANDARDISATION</h3><h6 style="text-align: center;"><strong>Frameworks</strong></h6><p><strong>MultiMedia framework</strong></p><p>Software heart of the platform will be multimedia framework. We look for the realtime computation, clean API, easy integration on libraries level, high coverage of formats and codecs. There are many of frameworks which allows to process media in very sophisticated manner:</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="OpenMAX" href="https://en.wikipedia.org/wiki/OpenMAX">OpenMAX</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="GStreamer" href="https://gstreamer.freedesktop.org/">GStreamer</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="FFmpeg" href="https://www.ffmpeg.org/">FFmpeg</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="VLC" href="http://www.videolan.org/index.html">VLC</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="DirectShow" href="https://en.wikipedia.org/wiki/DirectShow">DirectShow</a></li></ul><p>From my personal experience the best would be cross platform ready and open source GStreamer. Mainly because it implements clean separation between core and tons of plugins, is easy to extend with custom plugins and provides wrapper for almost every other media framework like OMX, FFmpeg and technologies in area of 2D/3D processing and computing vision.</p><p><strong>3D Graphic framework</strong></p><p>3D graphics have become so popular, particularly in <a title="Video game" href="https://en.wikipedia.org/wiki/Video_game">video games</a>, that specialized <a title="Application programming interface" href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a> (application programming interfaces) have been created to ease the processes in all stages of computer graphics generation. These APIs have also proved vital to computer graphics hardware manufacturers, as they provide a way for <a title="Programmer" href="https://en.wikipedia.org/wiki/Programmer">programmers</a> to access the hardware in an abstract way, while still taking advantage of the special hardware of any specific <a title="Graphics card" href="https://en.wikipedia.org/wiki/Graphics_card">graphics card</a>.</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="OpenGL" href="https://en.wikipedia.org/wiki/OpenGL">OpenGL</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="OpenGL" href="https://en.wikipedia.org/wiki/Direct3D">Direct3D</a></li></ul><p>For current project I will use OpenGL as I have gain enough of experience in that area. Also there is very good GStreamer (multimedia framework) integration.</p><p><strong>Computer vision framework</strong></p><p>Computer vision is a field that includes methods for <a title="Image sensor" href="https://en.wikipedia.org/wiki/Image_sensor">acquiring</a>, <a title="Digital image processing" href="https://en.wikipedia.org/wiki/Digital_image_processing">processing</a>, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, <i>e.g.</i>, in the forms of decisions. Understanding in this context means the transformation of visual images (the input of retina) into descriptions of world that can interface with other thought processes and elicit appropriate action</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="OpenCV" href="https://en.wikipedia.org/wiki/OpenCV">OpenCV</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/OpenVX">OpenVX</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://developer.nvidia.com/embedded/visionworks">VisionWorks</a></li></ul><p>This technology is extremely important for implementing "Intelligent Chroma Keying" which is the top goal of the challenge. I will use OpenCV as I have gain enough of experience in that area. Also there is good support with media frameworks and hardware acceleration (with GStreamer, CUDA).</p><p><strong>Parallel computing framework</strong></p><p>Parallel computing is a type of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved at the same time.</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="CUDA" href="https://en.wikipedia.org/wiki/CUDA">CUDA</a></li><li><a style="font-family: sans-serif; font-style: normal;" title="OpenCL" href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a></li></ul><p>In this challenge, extremely high computation in parallel will be used scene recognition, objects tracking and accurate segmentation.</p><p><strong>Neural Network framework</strong></p><p>Deep learning (deep structured learning, hierarchical learning or deep machine learning) is a branch of <a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> based on a set of <a title="Algorithm" href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-<a title="Linear transformation" href="https://en.wikipedia.org/wiki/Linear_transformation">linear transformations</a>.</p><p>Various deep learning architectures such as <a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks">deep neural networks</a>, <a title="Convolutional neural network" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional deep neural networks</a>, <a title="Deep belief network" href="https://en.wikipedia.org/wiki/Deep_belief_network">deep belief networks</a> and <a title="Recurrent neural network" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> have been applied to fields like <a title="Computer vision" href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, <a title="Automatic speech recognition" href="https://en.wikipedia.org/wiki/Automatic_speech_recognition">automatic speech recognition</a>,<a title="Natural language processing" href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, audio recognition and <a title="Bioinformatics" href="https://en.wikipedia.org/wiki/Bioinformatics">bioinformatics</a> where they have been shown to produce state-of-the-art results on various tasks.</p><ul><li><a style="font-family: sans-serif; font-style: normal;" title="CUDA" href="https://developer.nvidia.com/cudnn">cuDNN</a></li></ul><p>In this challenge the goal would be to use high efficient GPU to solve more complex problem of scene segmentation by continues learning.</p><h6 style="text-align: center;"> </h6><h6 style="text-align: center;"><strong>STANDARDISATION</strong></h6><p><strong>Digital video standards</strong></p><ul><li>4K, HD, FullHD, UltraHD</li><li>Ratio: 16:9, 4:3</li><li>Colors: RGB, YUV, YUVA, RGBA etc.</li><li>Format: RAW, MPEG</li></ul><p><strong>Network protocols</strong></p><p>The device will be fully controlled over network interface. It will allow to manage configuration, read state and push media data. The stack which we are gonna to use:</p><ul><li>RTP + (de)payload, WebRTC - multimedia streaming and communication</li><li>HTTP, WebSocket - configuration, signaling and API</li><li>STUN/TURN - support for WebRTC technologies and RTP</li></ul><p><strong>Operating System</strong></p><ul><li>Linux distribution: Ubuntu recommended. Why? It provides software packages for all technologies required by project.</li><li>Technologies: <a href="https://en.wikipedia.org/wiki/Video_Acceleration_API">VAAPI</a>, <a href="https://en.wikipedia.org/wiki/VDPAU">VDPAU</a>, <a href="https://en.wikipedia.org/wiki/Libav">AV</a>, <a href="https://en.wikipedia.org/wiki/GLX">GLX</a> required</li></ul><p><strong>Controlling</strong></p><p>Another piece of software we will have to handle is the management and controlling. This is relatively easy part which will provide the business logic for controlling the multimedia pipeline from outside the device. We can consider multiple approaches but the most common one will be:</p><ul><li>REST API</li><li>Web UI</li><li>CLI tool</li></ul><p><a name="infrastructure"></a></p><h3 style="text-align: center;">INFRASTRUCTURE AND SCALABILITY</h3><p><strong>Computing power distribution</strong></p><p>Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.</p><p>In case of video processing/distribution we have to introduce more sophisticated controlling rules. On one level we have to clone media and distribute on the topology and on another level we have to take care of signaling and media pipeline controlling.</p><p>I would suggest to consider following concepts:</p><ul><li>Horizontally via Video signal (HDMI, SDI, CSI-2)</li><li>Vertically via GPU opengl/shaders (Nvidia, AMD, Intel)</li><li>Vertically via Hardware acceleration (CPU/Skylake)</li></ul><p>Basically we have 2 ways to scale: horizontal which is more preferable and vertical which has many limitations. Each methods introduce hardware and controlling complexity.</p><p>In my opinion the "Horizontally via Video signal" is the best option to scale "infinitely".</p><p><strong>Provisioning and management</strong></p><p>IT automation software that helps system administrators manage infrastructure throughout its lifecycle, from provisioning and configuration to patch management and compliance. Using management tools, you can easily automate repetitive tasks, quickly deploy critical applications, and proactively manage change.</p><ul><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Chef_(software)">chef</a></li><li><a style="font-family: sans-serif; font-style: normal;" href="https://en.wikipedia.org/wiki/Puppet_(software)">puppet</a></li></ul><p><a name="summary"></a></p><h3 style="text-align: center;">SUMMARY</h3><p><strong>What is the best combination?</strong></p><p>Finally we went through tons of science, techniques, technologies, software and hardware! We can transform the initial abstract chart (with segments) into more detailed workflow of information (scene) through hardware and software up to the playback system.</p><p>Presented diagram shows stereoscopic input, business logic and output via digital media interface or network protocols:</p><figure class="post__image post__image--center"><img loading="lazy" src="https://blog.kris-lab.com/media/posts/889/F6suP4MGVC3QWCrD0nEFCJSUf7N4FCGRXj5UwepcX-vQHHA2-KNEXAo-gHHTP78oipnJkEa61QUhaf06KYrOmKhnRum2InzswzAd_5Vn6-X8WVYxM550QFD0F6l5b-_Obx7ZW0bm" alt="" width="624" height="303"></figure><p>In the following blogposts/articles I will focus on each part in details on the hardware and the software level.</p><p><strong>Next step</strong></p><p>During the next week I will focus on looking into the performance characteristics and development kits (CPU, GPU etc) currently available on the market...</p><p>Topic: <a href="https://blog.kris-lab.com/art-3dvr-challenge-week-2-looking-for-performance/" title="Art 3D/VR challenge – week 2 – looking for performance">Art 3D/VR challenge – week 2 – Looking for performance</a></p><p><strong>Contribution</strong></p><p>Feel free to contact me if you are interested in meeting the team and contribution to this project in any programming language (go, php, ruby, js, node.js, objective-c, java...). This project is parked on Github.</p><p>See <a href="https://blog.kris-lab.com/contact/">my contact page</a> if required.</p><p><strong>Resources</strong></p><p>Multiple parts of my blogpost have its source in remote articles, blogposts and wiki for which I have no rights. I am not able to link all external sources to my blogpost. I would like to say thank you to everyone who shares the knowledge publicly. If you think I have illegally used any of your thoughts, products, patents please let me know and I will fix the issue asap.</p><p>© COPYRIGHT KRZYSZTOF STASIAK 2016. ALL RIGHTS RESERVED</p></div><footer><p class="post__last-updated">This article was updated on October 21, 2020</p><div class="post__tags-share"><ul class="post__tag"><li><a href="https://blog.kris-lab.com/tag/camera/">Camera</a></li><li><a href="https://blog.kris-lab.com/tag/chihuahua/">Chihuahua</a></li><li><a href="https://blog.kris-lab.com/tag/chihuahua-vr/">Chihuahua-VR</a></li><li><a href="https://blog.kris-lab.com/tag/chihuahua-vr-box/">Chihuahua-VR box</a></li><li><a href="https://blog.kris-lab.com/tag/gpu/">GPU</a></li><li><a href="https://blog.kris-lab.com/tag/jet/">Jet</a></li><li><a href="https://blog.kris-lab.com/tag/jetson-tk1/">Jetson TK1</a></li><li><a href="https://blog.kris-lab.com/tag/jetson-tx1/">Jetson TX1</a></li><li><a href="https://blog.kris-lab.com/tag/real-time/">real-time</a></li><li><a href="https://blog.kris-lab.com/tag/stereoscopic/">Stereoscopic</a></li><li><a href="https://blog.kris-lab.com/tag/virtual-reality-2/">virtual reality</a></li><li><a href="https://blog.kris-lab.com/tag/vr/">VR</a></li><li><a href="https://blog.kris-lab.com/tag/vr3d/">VR/3D</a></li></ul><aside class="post__share"><a href="https://twitter.com/share?url=https%3A%2F%2Fblog.kris-lab.com%2Fart-3dvr-challenge-week-1-abstract-overview%2F&amp;via=%40_krislab&amp;text=Art%203D%2FVR%20challenge%20-%20week%201%20-%20abstract%20overview" class="js-share twitter" aria-label="Share with Twitter" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fblog.kris-lab.com%2Fart-3dvr-challenge-week-1-abstract-overview%2F&amp;media=https%3A%2F%2Fblog.kris-lab.com%2Fmedia%2Fposts%2F889%2FgxzP_VzJt2MEOFIJ6rEKNlduEqgnL-6UPZuHFyySQWjUQh8JokCK4Ug_vrppOVbCYM5SrQOS7xPJE8oQ7qWJyr9KmfLR4bwEOyk5ay8pt_NxO6f3B1bmWLGyY-7p6yx1PT9Gjaqd.png&amp;description=Art%203D%2FVR%20challenge%20-%20week%201%20-%20abstract%20overview" class="js-share pinterest" aria-label="Share with Pinterest" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#pinterest"/></svg> </a><a href="http://www.linkedin.com/shareArticle?url=https%3A%2F%2Fblog.kris-lab.com%2Fart-3dvr-challenge-week-1-abstract-overview%2F&amp;title=Art%203D%2FVR%20challenge%20-%20week%201%20-%20abstract%20overview" class="js-share linkedin" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#linkedin"/></svg> </a><a href="https://buffer.com/add?text=Art%203D%2FVR%20challenge%20-%20week%201%20-%20abstract%20overview&amp;url=https%3A%2F%2Fblog.kris-lab.com%2Fart-3dvr-challenge-week-1-abstract-overview%2F" class="js-share buffer" aria-label="Share with Buffer" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#buffer"/></svg> </a><a href="https://api.whatsapp.com/send?text=Art%203D%2FVR%20challenge%20-%20week%201%20-%20abstract%20overview https%3A%2F%2Fblog.kris-lab.com%2Fart-3dvr-challenge-week-1-abstract-overview%2F" class="js-share whatsapp" aria-label="Share with WhatsApp" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#whatsapp"/></svg></a></aside></div><div class="post__bio author"><div><h3 class="h6 author__name"><a href="https://blog.kris-lab.com/authors/chris-stasiak/" class="invert" title="Chris Stasiak">Chris Stasiak</a></h3></div></div><nav class="post__nav"><div class="post__nav__prev"><a class="post__nav__link" href="https://blog.kris-lab.com/art-challenge-with-ultrahd4k-3dvr-hardware-accelerated-relatime-hdmirtpwebrtc-live-streaming-and-communication-portable-device/" rel="prev">Previous Post<h3 class="h6">ART-Challenge with UltraHD/4K + 3D/VR hardware accelerated, relatime HDMI/RTP/WebRTC live streaming and communication - portable device</h3></a></div><div class="post__nav__next"><a class="post__nav__link" href="https://blog.kris-lab.com/art-3dvr-challenge-week-2-looking-for-performance/" rel="prev">Next Post<h3 class="h6">Art 3D/VR challenge – week 2 - looking for performance</h3></a></div></nav></footer></div></article><div class="comments post__inner" id="comments"><h3 class="h5">Comments</h3><div id="disqus_thread"></div><script>var disqus_config = function () {
                    this.page.url = '';
            		this.page.identifier = '';
                };
            
                var disqus_loaded = false;
            
                function publiiLoadDisqus() {
                    if(disqus_loaded) {
                        return false;
                    }
            
                    var top = document.getElementById('disqus_thread').offsetTop;
            
                    if (!disqus_loaded && (window.scrollY || window.pageYOffset) + window.innerHeight > top) {
                        disqus_loaded = true;
            
                        (function () {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://blog-kris-lab-com.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    }
                }
            
                publiiLoadDisqus();
            
                window.onscroll = function() {
                    publiiLoadDisqus();
                };</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></div></div><div class="post__related"><div class="wrapper"><h2 class="h5">Related posts</h2><div class="l-grid l-grid--4"><article class="c-card"><a href="https://blog.kris-lab.com/art-3dvr-challenge-jetson-tx1-module-with-auvidea-j100-carrier-lite-board/" class="c-card__image"><img src="https://blog.kris-lab.com/media/posts/890/WRVK1D8ejNPfH3TjXFKsbuVny9NymcW210AU_vx5fvsgjjlD0LG0Rr8tBmSAmvksazQC7DF76Wo9X_DKCeiPxVJJWRs_lONioe3IEjTgQiiI_Dx7ZZTbDL-DSwSR121AWr3Bq-u" loading="lazy" height="1600" width="1600" alt=""></a><div class="c-card__wrapper"><header class="c-card__header"><div class="c-card__tag"><a href="https://blog.kris-lab.com/tag/auvidea/">Auvidea</a></div><h3 class="c-card__title"><a href="https://blog.kris-lab.com/art-3dvr-challenge-jetson-tx1-module-with-auvidea-j100-carrier-lite-board/" class="invert">Art 3D/VR challenge – Jetson TX1 module with Auvidea J100 carrier lite board</a></h3></header><p class="c-card__text">Jetson TX1 works perfectly when mounted on Auvidea J100 carrier board. It is powered with 12V, starts automatically and boots without any troubles. TX1 + J100 is fully independent platform. Please see the photo below which presents working setup with&hellip;</p><footer class="c-card__meta"><time datetime="2016-04-26T19:41">April 26, 2016</time></footer></div></article><article class="c-card"><a href="https://blog.kris-lab.com/art-3dvr-challenge-more-parts/" class="c-card__image"><img src="https://blog.kris-lab.com/media/posts/888/dcd2eb933754229bac5c65277bf296e1.jpeg" srcset="https://blog.kris-lab.com/media/posts/888/responsive/dcd2eb933754229bac5c65277bf296e1-xs.jpeg 300w, https://blog.kris-lab.com/media/posts/888/responsive/dcd2eb933754229bac5c65277bf296e1-sm.jpeg 480w, https://blog.kris-lab.com/media/posts/888/responsive/dcd2eb933754229bac5c65277bf296e1-md.jpeg 768w, https://blog.kris-lab.com/media/posts/888/responsive/dcd2eb933754229bac5c65277bf296e1-lg.jpeg 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="lazy" height="500" width="500" alt=""></a><div class="c-card__wrapper"><header class="c-card__header"><div class="c-card__tag"><a href="https://blog.kris-lab.com/tag/audio-2/">audio</a></div><h3 class="c-card__title"><a href="https://blog.kris-lab.com/art-3dvr-challenge-more-parts/" class="invert">Art 3D/VR challenge - more parts</a></h3></header><p class="c-card__text">A very interesting audio device has arrived today. This is USB2.0 audio card based on chipset from C-Media Electronics Inc. which supports stereo output and input with sampling 44.1k and 48k. Tested, it works with Linux kernel 3.19 out of&hellip;</p><footer class="c-card__meta"><time datetime="2016-04-25T19:31">April 25, 2016</time></footer></div></article><article class="c-card"><a href="https://blog.kris-lab.com/art-3dvr-challenge-a-star-has-arrived/" class="c-card__image"><img src="https://blog.kris-lab.com/media/posts/887/Jetson_TX1_Press_Deck_Final-page-005.jpg" srcset="https://blog.kris-lab.com/media/posts/887/responsive/Jetson_TX1_Press_Deck_Final-page-005-xs.jpg 300w, https://blog.kris-lab.com/media/posts/887/responsive/Jetson_TX1_Press_Deck_Final-page-005-sm.jpg 480w, https://blog.kris-lab.com/media/posts/887/responsive/Jetson_TX1_Press_Deck_Final-page-005-md.jpg 768w, https://blog.kris-lab.com/media/posts/887/responsive/Jetson_TX1_Press_Deck_Final-page-005-lg.jpg 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="lazy" height="2025" width="3600" alt=""></a><div class="c-card__wrapper"><header class="c-card__header"><div class="c-card__tag"><a href="https://blog.kris-lab.com/tag/audio-card/">audio card</a></div><h3 class="c-card__title"><a href="https://blog.kris-lab.com/art-3dvr-challenge-a-star-has-arrived/" class="invert">Art 3D/VR challenge – a star has arrived</a></h3></header><p class="c-card__text">Finally, the Jetson TX1 has arrived; to buy one in Europe it took me at least 4 weeks more than expected, but let's see what we can squeeze out of this tiny Nvidia's baby... Booted, upgraded, connected to Wifi, tested&hellip;</p><footer class="c-card__meta"><time datetime="2016-04-26T13:06">April 26, 2016</time></footer></div></article><article class="c-card"><div class="c-card__wrapper"><header class="c-card__header"><div class="c-card__tag"><a href="https://blog.kris-lab.com/tag/chihuahua/">Chihuahua</a></div><h3 class="c-card__title"><a href="https://blog.kris-lab.com/art-3dvr-challenge-waiting-for-missing-parts/" class="invert">Art 3D/VR challenge - waiting for missing parts...</a></h3></header><p class="c-card__text">Quick update I am still waiting for many parts for the VR project. Have a look on the list and photos of stuff I have already received: All parts seem to be working very well. I have already done a&hellip;</p><footer class="c-card__meta"><time datetime="2016-04-21T18:51">April 21, 2016</time></footer></div></article></div></div></div></main><footer class="footer"><div class="footer__social"><a href="https://twitter.com/_krislab" aria-label="Twitter" class="twitter"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.instagram.com/_krislab" aria-label="Instagram" class="instagram"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/krzysztof-chris-stasiak-6261244b/" aria-label="LinkedIn" class="linkedin"><svg><use xlink:href="https://blog.kris-lab.com/assets/svg/svg-map.svg#linkedin"/></svg></a></div><div class="footer__copyright">&copy; 2020 Krzysztof Stasiak</div></footer><script>window.publiiThemeMenuConfig = {    
      mobileMenuMode: 'sidebar',
      animationSpeed: 300,
      submenuWidth: 'auto',
      doubleClickTime: 500,
      mobileMenuExpandableSubmenus: true, 
      relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://blog.kris-lab.com/assets/js/scripts.min.js?v=8b3e67b51e8c7a7d51b33f4a94ae4287"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script><script id="dsq-count-scr" src="https://blog-kris-lab-com.disqus.com/count.js" async></script></body></html>